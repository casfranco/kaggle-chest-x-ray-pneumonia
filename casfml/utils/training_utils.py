from gc import callbacks
import tensorflow as tf
from matplotlib import pyplot as plt
from IPython.display import clear_output


def allow_growth(tf_version=1):
    """Allows gpu growth instead of consuming the whole GPU memory at once.
    """
    if(tf_version==1):
        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth=True
        sess = tf.compat.v1.Session(config=config)
    else:
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            try:
                # Currently, memory growth needs to be the same across GPUs
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)

                if(len(gpus) > 1 ):
                    #mirrored_strategy = tf.distribute.MirroredStrategy()
                    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')
                logical_gpus = tf.config.experimental.list_logical_devices('GPU')
                print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
                
            except RuntimeError as e:
                # Memory growth must be set before GPUs have been initialized
                print(e)

# ---------------------------------------------------------------------------- #
#                  callbacks for plot training learning curves                 #
# ---------------------------------------------------------------------------- #

class PlotLearning(tf.keras.callbacks.Callback):
    def __init__(self):
        plt.rcParams["figure.figsize"]=15,7
        
    def on_train_begin(self, logs={}):
        self.i = 0
        self.x = []
        self.losses = []
        self.val_losses = []
        self.acc = []
        self.val_acc = []
        self.fig = plt.figure()
        
        self.logs = []

    def on_epoch_end(self, epoch, logs={}):
        
        self.logs.append(logs)
        self.x.append(self.i)
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))
        self.acc.append(logs.get('accuracy'))
        self.val_acc.append(logs.get('val_accuracy'))
        self.i += 1
        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)
        
        clear_output(wait=True)
        
        ax1.set_yscale('log')
        ax1.plot(self.x, self.losses, label="loss")
        ax1.plot(self.x, self.val_losses, label="val_loss")
        ax1.legend()
        
        ax2.plot(self.x, self.acc, label="accuracy")
        ax2.plot(self.x, self.val_acc, label="validation accuracy")
        ax2.legend()
        
        plt.show();

class PlotLosses(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.i = 0
        self.x = []
        self.losses = []
        self.val_losses = []
        
        self.fig = plt.figure()
        
        self.logs = []

    def on_epoch_end(self, epoch, logs={}):
        
        self.logs.append(logs)
        self.x.append(self.i)
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))
        self.i += 1
        
        clear_output(wait=True)
        plt.plot(self.x, self.losses, label="loss")
        plt.plot(self.x, self.val_losses, label="val_loss")
        plt.legend()
        plt.show();
